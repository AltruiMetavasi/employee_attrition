---
title: "Untitled"
author: "Shelloren"
date: "November 5, 2018"
output: html_document
---
---
title : "HR Analytic" 
output:
  prettydoc::html_pretty:
  theme: cayman
  highlight: github
---
# Employee Attrition {.tabset .tabset-fade}

## Background
  Attrition in very basic concept is type of employee churn. Some probably wonders what the diffrent with another type of churn 'the turnover', both are a decrease number of employees on staff, but attrition is typically voluntary or natural - like retirement or resignation.
  
  The problem is this could lead to relatively high cost to the company, the time or the cost of money from acquiring a new talent. In fact, [the average cost-per-hire to fill a vacant position due to turnover or preventable attrition is $4,129](https://www.shrm.org/hr-today/trends-and-forecasting/research-and-surveys/Documents/2017-Talent-Acquisition-Benchmarking.pdf).  
  
  In this project I try to predict employee attrition with machine learning. I will use a data set provide by [Watson Analytics Sample Data](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/). In his data, each variable (row) describes the employee with parameters like: age, department, Job Role, income, years at company, etc. The target variable 'Attrition' is known (it is historical value) and  our main objective is to do machine learning classification (we predict yes/no for attrition).

```{r setup, include=FALSE}
# Set up packages
pacman::p_load(tidyverse, tidyquant, ggplot2, gmodels, class, tm, mlr, partykit, ROCR, cluster, caret, parallel, doParallel, randomForest, prettydoc, kableExtra, corrplot, ISLR, cowplot)

# fix some crash in namespace
slice <- dplyr::slice
margin <- ggplot2::margin
```  

```{css}
<style>
div.blue pre { background-color:lightblue; }
div.blue pre.r { background-color:blue; }
</style>

<div class = "blue">

```

  The HR_employee_attrition.csv data is containing Employee Attrition and Performance from IBM, take a peek to the data:
```{r echo=FALSE, warning=FALSE}
# import data set
attdat <- read_csv("data_input/HR_employee_attrition.csv")

# building table
kable(attdat[1:5,]) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>% 
   scroll_box(width = "100%")
```


In here I will delve into the specific details with far greater depth few of
parameters in the dataset we've just read into our environment:  
+ `EmployeeID`: The ID of each individual employee.
+ `Attrition`: Yes or No Parameter wether the employee commit attrition.
+ `BusinessTravel`: One of factor of `Non-Travel`, `Travel_Frequently` and `Travel_Rarely`
+ `Education`: Ranged from 1 `Below College`, 2 `College`, 3 `Bachelor`, 4 `Master` and 5 `Doctor ` 
+ `EnvironmentSatisfaction`:Rating from employee range from; 1 `Low`, 2 `Medium`, 3 `High` and 4 `Very High`. This appraisal were included in few parameters such as;`JobInvolvement`,`RelationshipSatisfaction`, `PerformanceRating`, `JobSatisfaction` and `WorkLifeBalance`.

## Preprocessing data

  Before we start anylizing the data we should check if theres mising value that would hinder our process.
```{r}
# detecting missing value
attdat %>% 
  is.na() %>% 
  colnames()

```
  
  
  There's no missing value and after we check, there's a few parameters in the data who don't have variance that we could remov it in order to decrease dimension.
```{r results='hide'}
# Quick check 
glimpse(attdat) 

# Drop no-Variance Variables
attdat %<>%
  select(-EmployeeCount, -EmployeeNumber, -Over18, -StandardHours) %>%
  mutate(
    Attrition = factor(Attrition, levels = c("Yes", "No")),
    Age = Age %>% as.numeric(),
    DailyRate = DailyRate %>% as.numeric(),
    DistanceFromHome = DistanceFromHome %>% as.numeric(),
    HourlyRate = HourlyRate %>% as.numeric(),
    MonthlyIncome = MonthlyIncome %>% as.numeric(),
    MonthlyRate = MonthlyRate %>% as.numeric(),
    NumCompaniesWorked = NumCompaniesWorked %>% as.numeric(),
    PercentSalaryHike = PercentSalaryHike %>% as.numeric(),
    TotalWorkingYears = TotalWorkingYears %>% as.numeric(),
    TrainingTimesLastYear = TrainingTimesLastYear %>% as.numeric(),
    YearsAtCompany = YearsAtCompany %>% as.numeric(),
    YearsInCurrentRole = YearsInCurrentRole %>% as.numeric(),
    YearsSinceLastPromotion = YearsSinceLastPromotion %>% as.numeric(),
    YearsWithCurrManager = YearsWithCurrManager %>% as.numeric(),
    EmployeeID = row_number()
  ) %>%
  mutate_if(is.character, as.factor) %>%
  mutate_if(is.integer, as.factor) %>%
  select(EmployeeID, everything())

# save final dataset for Shiny
saveRDS(attdat, "data_input/attrition.RDS")
write_csv(attdat, "data_input/predDataTemplate.csv")
```

  We split the data to training set that contains a known output and the model learns on this data in order to be generalized to other data later on and  the test dataset (or subset) in order to test our model's prediction on this subset.
```{r}
# Spliting the data
set.seed(100)
inTrain <- createDataPartition(attdat$Attrition, p = 0.9, list = FALSE)
trainatt <- attdat %>% slice(inTrain)
testatt <- attdat %>% slice(-inTrain)

# Check the portion of Attrition
table(trainatt$Attrition)
```

  As we could see there's imbalance data. I do [subsampling](https://topepo.github.io/caret/subsampling-for-class-imbalances.html#methods), with downsampling thechnique which randomly subset all the classes in the training set so that their class frequencies match the least prevalent class.
```{r}
# down sampling the train data
set.seed(100)
down_train <- downSample(x = trainatt[, -1],
                         y = trainatt$Attrition) %>% 
              select(-Class)

# down sampling the test data
set.seed(100)
down_test <- downSample(x = testatt[, -1],
                         y = testatt$Attrition) %>% 
             select(-Class)
```

  Because later on i would do standard classifier algorithms like Logistic Regression and Random Forest that have a bias towards classes which have number of instances. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class.

So we do undersampling aims to balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out.
```{r echo=FALSE}
# plotting Age reflect on attrition
plotA <- ggplot(attdat, aes(x=Age, fill = Attrition)) +
  geom_density(alpha=0.4, colour = NA)+
  ggtitle("Age frequency on Attrition") +
  scale_fill_tq() +
  theme_tq()

# plotting Business Travel reflect on Attrition
plotBT <- attdat %>%
  filter(Attrition == "Yes") %>% 
  ggplot(aes(x = Age, fill = BusinessTravel)) +
  geom_density(alpha = 0.4, colour = NA) +
  labs(
    x = "Age",
    y = "Business Travel"
  ) +
  scale_fill_tq() +
  theme_tq()

plot_grid(plotA, plotBT, labels = c('A', 'B'))
